{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "989e6d06",
   "metadata": {},
   "source": [
    "## Prerequisities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "062eb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import pygame\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c60a85",
   "metadata": {},
   "source": [
    "## Custom Maze Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6808f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates a 5x5 maze environment with random placements for the goal , mines and agnet. it's a stochastic environment where the agent will move to the \n",
    "# intended direction with probability 70% and to each of the perpendicular directions with probability 15%\n",
    "\n",
    "class GridMazeEnv(gym.Env):\n",
    "   \n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, grid_size=5, render_mode=\"rgb_array\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the grid layout\n",
    "        self.grid_size = grid_size\n",
    "        self.cell_size = 100\n",
    "        self.width = self.grid_size * self.cell_size\n",
    "        self.height = self.grid_size * self.cell_size\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Define the action space: 4 discrete actions (0:Up, 1:Right, 2:Down, 3:Left)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.action_to_direction = {0: (-1, 0), 1: (0, 1), 2: (1, 0), 3: (0, -1)}\n",
    "        self.observation_space = spaces.MultiDiscrete([grid_size, grid_size])\n",
    "\n",
    "        # PyGame setup for rendering\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "\n",
    "    # This function takes a seed, shuffles the coordinates of the grid and returns 4 distinct random coordinates : 1 for the goal, 2 for the mines and 1 for the agent\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Use super().reset(seed=seed) for proper seeding of environment's RNG\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        layout_seed = options.get('layout_seed', None) if options else None\n",
    "        agent_seed = options.get('agent_seed', None) if options else None\n",
    "\n",
    "        # Use separate RNGs for layout and agent start\n",
    "        layout_rng = np.random.default_rng(layout_seed)\n",
    "        agent_rng = np.random.default_rng(agent_seed) if agent_seed is not None else self.np_random\n",
    "\n",
    "        # --- Generate Layout (Goal and Non-Adjacent Mines) ---\n",
    "        \n",
    "        # Get all possible cell coordinates\n",
    "        all_cells = [(r, c) for r in range(self.grid_size) for c in range(self.grid_size)]\n",
    "        \n",
    "        # Shuffle the list to pick randomly without replacement\n",
    "        layout_rng.shuffle(all_cells) \n",
    "\n",
    "        # Pick Goal\n",
    "        self.goal_pos = all_cells.pop() # Takes the last element after shuffle\n",
    "\n",
    "        # Pick First Mine\n",
    "        mine1_pos = all_cells.pop() # Takes the new last element\n",
    "\n",
    "        # Pick Second Mine, ensuring it's not adjacent to the first\n",
    "        mine2_pos = None\n",
    "        # Create a list of indices into the *current* all_cells list\n",
    "        eligible_mine2_indices = list(range(len(all_cells))) \n",
    "        layout_rng.shuffle(eligible_mine2_indices) # Shuffle indices to try randomly\n",
    "        \n",
    "        found_mine2 = False\n",
    "        for idx_in_list in eligible_mine2_indices:\n",
    "            potential_mine2_pos = all_cells[idx_in_list]\n",
    "            # Check Manhattan distance: |r1-r2| + |c1-c2| == 1 means adjacent\n",
    "            if abs(potential_mine2_pos[0] - mine1_pos[0]) + abs(potential_mine2_pos[1] - mine1_pos[1]) != 1:\n",
    "                # Found a non-adjacent position\n",
    "                mine2_pos = potential_mine2_pos\n",
    "                # Remove the chosen mine from the list using its index *before* popping\n",
    "                all_cells.pop(idx_in_list) \n",
    "                found_mine2 = True\n",
    "                break # Exit the loop once a valid position is found\n",
    "\n",
    "        # Safety check (shouldn't happen in a 5x5 grid unless almost full)\n",
    "        if not found_mine2:\n",
    "            raise RuntimeError(\"Could not place two non-adjacent mines. Grid might be too small or logic error.\")\n",
    "\n",
    "        self.mines = [mine1_pos, mine2_pos]\n",
    "\n",
    "        # --- Agent Placement ---\n",
    "        # Pick Agent Start from the remaining cells using the agent RNG\n",
    "        agent_idx_in_list = agent_rng.choice(len(all_cells))\n",
    "        self.agent_pos = np.array(all_cells[agent_idx_in_list])\n",
    "\n",
    "        # --- Debug Prints (Optional) ---\n",
    "        print(\"goal pos = \", self.goal_pos)\n",
    "        print(\"mines pos = \", self.mines)\n",
    "        print(\"agent pos = \", self.agent_pos)\n",
    "\n",
    "        return self.agent_pos, {}\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        # 70% intended, 15% perpendicular left, 15% perpendicular right\n",
    "        outcomes = []\n",
    "        if action == 0:  # Up\n",
    "            outcomes = [0, 3, 1] # Actual actions: Up, Left, Right\n",
    "        elif action == 1: # Right\n",
    "            outcomes = [1, 0, 2] # Actual actions: Right, Up, Down\n",
    "        elif action == 2: # Down\n",
    "            outcomes = [2, 1, 3] # Actual actions: Down, Right, Left\n",
    "        elif action == 3: # Left\n",
    "            outcomes = [3, 2, 0] # Actual actions: Left, Down, Up\n",
    "\n",
    "        # Choose the actual move based on probabilities\n",
    "        actual_action = np.random.choice(outcomes, p=[0.7, 0.15, 0.15])\n",
    "        direction = self.action_to_direction[actual_action]\n",
    "\n",
    "        # Update agent position, ensuring it stays within the grid\n",
    "        self.agent_pos = np.clip(self.agent_pos + direction, 0, self.grid_size - 1)\n",
    "\n",
    "        # Determine reward and if the episode is done\n",
    "        terminated = False\n",
    "        if np.array_equal(self.agent_pos, self.goal_pos):\n",
    "            reward = 10.0\n",
    "            terminated = True\n",
    "        elif tuple(self.agent_pos) in self.mines:\n",
    "            reward = -50.0\n",
    "            terminated = True\n",
    "        else:\n",
    "            # Small negative reward to encourage finding the goal faster\n",
    "            reward = -0.1\n",
    "\n",
    "        truncated = False # No time limit in this environment\n",
    "        info = {}\n",
    "\n",
    "        return self.agent_pos, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            if self.render_mode == \"human\":\n",
    "                self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "                pygame.display.set_caption(\"Grid Maze\")\n",
    "            else:  # rgb_array mode\n",
    "                self.screen = pygame.Surface((self.width, self.height))\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        # Fill background\n",
    "        self.screen.fill((255, 255, 255))\n",
    "\n",
    "        # Draw Goal (Green)\n",
    "        goal_rect = pygame.Rect(self.goal_pos[1] * self.cell_size, self.goal_pos[0] * self.cell_size, self.cell_size, self.cell_size)\n",
    "        pygame.draw.rect(self.screen, (0, 255, 0), goal_rect)\n",
    "\n",
    "        # Draw Mines (Red)\n",
    "        for mine in self.mines:\n",
    "            mine_rect = pygame.Rect(mine[1] * self.cell_size, mine[0] * self.cell_size, self.cell_size, self.cell_size)\n",
    "            pygame.draw.rect(self.screen, (255, 0, 0), mine_rect)\n",
    "\n",
    "        # Draw Agent (Blue Circle)\n",
    "        center_x = self.agent_pos[1] * self.cell_size + self.cell_size // 2\n",
    "        center_y = self.agent_pos[0] * self.cell_size + self.cell_size // 2\n",
    "        pygame.draw.circle(self.screen, (0, 0, 255), (center_x, center_y), self.cell_size // 3)\n",
    "\n",
    "        # Draw grid lines\n",
    "        for x in range(0, self.width, self.cell_size):\n",
    "            pygame.draw.line(self.screen, (0, 0, 0), (x, 0), (x, self.height))\n",
    "        for y in range(0, self.height, self.cell_size):\n",
    "            pygame.draw.line(self.screen, (0, 0, 0), (0, y), (self.width, y))\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.display.flip()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            return np.transpose(pygame.surfarray.array3d(self.screen), (1, 0, 2))\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            pygame.quit()\n",
    "            self.screen = None\n",
    "            self.clock = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffcc10c",
   "metadata": {},
   "source": [
    "## Policy iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d1a0e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function executes the policy iteration with its 2 phases : policy evaluation and policy improvement\n",
    "\n",
    "class PolicyIterationAgent:\n",
    "    def __init__(self, env, gamma=0.9, theta=1e-6):\n",
    "        self.env = env\n",
    "        self.num_states = env.grid_size * env.grid_size\n",
    "        self.num_actions = env.action_space.n   \n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.theta = theta  # Convergence threshold\n",
    "\n",
    "        # Initialize a random policy and a zero value function\n",
    "        # The policy is a mapping from state_index -> action\n",
    "        self.policy = np.random.randint(self.num_actions, size=self.num_states)\n",
    "        # The value function is a table of values for each state\n",
    "        self.value_function = np.zeros(self.num_states)\n",
    "\n",
    "    def _state_to_index(self, state):\n",
    "        #Converts a (row, col) state to a single index.\n",
    "        # multiply the row by thegrid size (here 5) then add the column such that all cells are enumerated from 0 to 24 (like the example in the lecture slides) \n",
    "        return state[0] * self.env.grid_size + state[1]\n",
    "\n",
    "    def _get_transition_model(self):\n",
    "        # Computes the model of the environment: P(s' | s, a) (probability enni lamma ab2a f state s wakhod action a awsal l state s') \n",
    "        # and R(s, a, s') ( lamma ab2a f state s wakhod action a awsal l state s' hakhod reward ad eih).\n",
    "        # This is a key step for model-based DP algorithms like Policy Iteration.\n",
    "        model = {}\n",
    "        for row in range(self.env.grid_size):\n",
    "            for col in range(self.env.grid_size):\n",
    "                state_idx = self._state_to_index((row, col))   # the number of the cell in question\n",
    "                model[state_idx] = {a: [] for a in range(self.num_actions)}   # get all the actions available for this cell\n",
    "\n",
    "                # If the state is terminal, all actions lead back to it with 0 reward.\n",
    "                if (row, col) == tuple(self.env.goal_pos) or (row, col) in self.env.mines:\n",
    "                    for a in range(self.num_actions):\n",
    "                        model[state_idx][a].append((1.0, state_idx, 0.0))\n",
    "                    continue\n",
    "\n",
    "                for action in range(self.num_actions):\n",
    "                    # Define stochastic outcomes\n",
    "                    outcomes = []\n",
    "                    if action == 0: outcomes = [(0.7, 0), (0.15, 3), (0.15, 1)] # Up, Left, Right\n",
    "                    elif action == 1: outcomes = [(0.7, 1), (0.15, 0), (0.15, 2)] # Right, Up, Down\n",
    "                    elif action == 2: outcomes = [(0.7, 2), (0.15, 1), (0.15, 3)] # Down, Right, Left\n",
    "                    elif action == 3: outcomes = [(0.7, 3), (0.15, 2), (0.15, 0)] # Left, Down, Up\n",
    "\n",
    "                    for prob, actual_action in outcomes:\n",
    "                        direction = self.env.action_to_direction[actual_action]\n",
    "                        next_pos = np.clip(np.array([row, col]) + direction, 0, self.env.grid_size - 1)\n",
    "                        next_state_idx = self._state_to_index(next_pos)\n",
    "\n",
    "                        # Calculate reward for this transition\n",
    "                        if tuple(next_pos) == tuple(self.env.goal_pos):\n",
    "                            reward = 10.0\n",
    "                        elif tuple(next_pos) in self.env.mines:\n",
    "                            reward = -50.0\n",
    "                        else:\n",
    "                            reward = -0.1\n",
    "\n",
    "                        model[state_idx][action].append((prob, next_state_idx, reward))\n",
    "        return model\n",
    "\n",
    "    def policy_evaluation(self):\n",
    "        # Calculates the value function for the current policy by iteratively applying the Bellman expectation equation until values converge.\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(self.num_states):\n",
    "                v = self.value_function[s]\n",
    "                action = self.policy[s]\n",
    "                new_v = 0\n",
    "                # Sum over possible next states\n",
    "                for prob, next_s, reward in self.transition_model[s][action]:\n",
    "                    # Bellman equation \n",
    "                    new_v += prob * (reward + self.gamma * self.value_function[next_s])\n",
    "                self.value_function[s] = new_v\n",
    "                delta = max(delta, abs(v - self.value_function[s]))\n",
    "\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        # Improves the policy by acting greedily with respect to the current value function. Returns True if the policy is stable, False otherwise.\n",
    "        policy_stable = True\n",
    "        for s in range(self.num_states):\n",
    "            old_action = self.policy[s]\n",
    "            action_values = np.zeros(self.num_actions)\n",
    "            for a in range(self.num_actions):\n",
    "                # Calculate the value of taking action 'a' in state 's'\n",
    "                for prob, next_s, reward in self.transition_model[s][a]:\n",
    "                    action_values[a] += prob * (reward + self.gamma * self.value_function[next_s])\n",
    "\n",
    "            # Choose the best action\n",
    "            best_action = np.argmax(action_values)\n",
    "            self.policy[s] = best_action\n",
    "\n",
    "            if old_action != best_action:\n",
    "                policy_stable = False\n",
    "        return policy_stable\n",
    "\n",
    "    def solve(self):\n",
    "        # Runs the main Policy Iteration loop until the policy converges.\n",
    "        print(\"Building transition model\")\n",
    "        self.transition_model = self._get_transition_model()\n",
    "        print(\"Model built.\")\n",
    "\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            iteration += 1\n",
    "            print(f\"\\n--- Iteration {iteration} ---\")\n",
    "            print(\"1. Evaluating policy...\")\n",
    "            self.policy_evaluation()\n",
    "\n",
    "            print(\"2. Improving policy...\")\n",
    "            policy_stable = self.policy_improvement()\n",
    "\n",
    "            if policy_stable:\n",
    "                print(\"\\nPolicy converged!\")\n",
    "                print(f\"Converged in {iteration} iterations.\")\n",
    "                break\n",
    "\n",
    "    def predict(self, state):\n",
    "        # Predicts the best action for a given state using the learned policy.\n",
    "        \n",
    "        state_idx = self._state_to_index(state)\n",
    "        return self.policy[state_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208d1de",
   "metadata": {},
   "source": [
    "## Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6168be34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal pos =  (4, 0)\n",
      "mines pos =  [(2, 3), (1, 0)]\n",
      "agent pos =  [4 1]\n",
      "Building transition model\n",
      "Model built.\n",
      "\n",
      "--- Iteration 1 ---\n",
      "1. Evaluating policy...\n",
      "2. Improving policy...\n",
      "\n",
      "--- Iteration 2 ---\n",
      "1. Evaluating policy...\n",
      "2. Improving policy...\n",
      "\n",
      "--- Iteration 3 ---\n",
      "1. Evaluating policy...\n",
      "2. Improving policy...\n",
      "\n",
      "--- Iteration 4 ---\n",
      "1. Evaluating policy...\n",
      "2. Improving policy...\n",
      "\n",
      "--- Iteration 5 ---\n",
      "1. Evaluating policy...\n",
      "2. Improving policy...\n",
      "\n",
      "--- Iteration 6 ---\n",
      "1. Evaluating policy...\n",
      "2. Improving policy...\n",
      "\n",
      "Policy converged!\n",
      "Converged in 6 iterations.\n",
      "Training time: 0.07 seconds\n",
      "\n",
      "Optimal Policy:\n",
      " ↑ | → | ↓ | ← | ← \n",
      " ↑ | → | ↓ | ↑ | ← \n",
      " ↓ | ↓ | ← | ↑ | → \n",
      " ↓ | ↓ | ← | ↓ | ↓ \n",
      " ↑ | ← | ← | ← | ← \n",
      "\n",
      "Recording video of the optimal policy in action...\n",
      "goal pos =  (4, 0)\n",
      "mines pos =  [(2, 3), (1, 0)]\n",
      "agent pos =  [2 4]\n",
      "Video saved in the 'videos' folder.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- 1. Training Phase ---\n",
    "    layout_seed= 54321\n",
    "    agent_seed_train= 12345\n",
    "    agent_seed_test= 98765\n",
    "    env = GridMazeEnv()\n",
    "    _ = env.reset(options = {\"layout_seed\": layout_seed, \"agent_seed\": agent_seed_train})  # Fixed maze layout\n",
    "\n",
    "    agent = PolicyIterationAgent(env, gamma=0.99)\n",
    "    start_time = time.time()\n",
    "    agent.solve()\n",
    "    end_time = time.time()\n",
    "    print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    # Display the final optimal policy\n",
    "    policy_grid = agent.policy.reshape(env.grid_size, env.grid_size)\n",
    "    action_symbols = {0: \"↑\", 1: \"→\", 2: \"↓\", 3: \"←\"}\n",
    "    print(\"\\nOptimal Policy:\")\n",
    "    for r in range(env.grid_size):\n",
    "        row_str = \" | \".join([action_symbols[p] for p in policy_grid[r]])\n",
    "        print(f\" {row_str} \")\n",
    "\n",
    "    # --- 2. Evaluation & Recording Phase ---\n",
    "    print(\"\\nRecording video of the optimal policy in action...\")\n",
    "\n",
    "    # Create new environment with the same maze layout (seed 37)\n",
    "    video_env = GridMazeEnv(render_mode=\"rgb_array\")\n",
    "    video_env = RecordVideo(video_env, video_folder=\"videos\", name_prefix=\"policy-iteration-solution\")\n",
    "\n",
    "    # Fix goal/mines using the same seed\n",
    "    obs, _ = video_env.reset(options = {\"layout_seed\": layout_seed, \"agent_seed\": agent_seed_test})\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = video_env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    video_env.close()\n",
    "    print(\"Video saved in the 'videos' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf3c887-5ee1-4702-8401-6e4f0f823d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0f88da-1362-4595-bb13-af8068b91bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
